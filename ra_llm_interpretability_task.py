# -*- coding: utf-8 -*-
"""RA - LLM Interpretability Task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gQh-kOQpo_Fm9E9zY3PQlyJUHqBoIX4o
"""

# Mount Google Drive and load dataset
from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Configure visualization
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# Mount drive and load data
drive.mount('/content/drive')
df = pd.read_csv('/content/drive/MyDrive/RA_Application_Task.csv')

df.head()

"""### 1. Data Validation & Quality Assessment"""

# Initial Inspection: Shape, Dtypes, Missing Values
print(f"Dataset Shape: {df.shape}")
print("\nData Types:")
print(df.dtypes)
print("\nMissing Values:")
print(df.isnull().sum())

# Verify zpid uniqueness
duplicate_mask = df.duplicated('Comparable zpid', keep=False)
duplicates = df[duplicate_mask]
num_duplicates = df.duplicated('Comparable zpid').sum()

print(f"Number of duplicate zpids: {num_duplicates}")
if num_duplicates > 0:
    print("\nDuplicate Entries (Preview):")
    display(duplicates.sort_values('Comparable zpid').head())

# Statistical summary
display(df.describe())

# Validate Value Ranges & Quality Issues
print("Quality Checks:")

# Prices positive
invalid_prices = df[df['Comparable Sale Price'] <= 0]
print(f"- Non-positive prices: {len(invalid_prices)}")

# Logical Bedrooms/Bathrooms
invalid_beds = df[df['Bedrooms'] < 0]
invalid_baths = df[df['Bathrooms Comparable'] < 0]
print(f"- Negative bedrooms: {len(invalid_beds)}")
print(f"- Negative bathrooms: {len(invalid_baths)}")

# Years valid (Checking against mentioned range 1925-1965)
year_col = 'YearBuilt Comparable'
out_of_range_years = df[(df[year_col] < 1925) | (df[year_col] > 1965)]
print(f"- Years outside 1925-1965: {len(out_of_range_years)}")
print(f"  (Min Year: {df[year_col].min()}, Max Year: {df[year_col].max()})")

# Zero lot sizes
zero_lot = df[df['LotSize Comparable'] <= 0]
print(f"- Zero/Negative Lot Sizes: {len(zero_lot)}")

"""### 2. Exploratory Data Analysis (EDA)"""

# Univariate Analysis: Price Distribution & Normality
plt.figure(figsize=(10, 5))
sns.histplot(df['Comparable Sale Price'], kde=True)
plt.title('Comparable Sale Price Distribution')
plt.xlabel('Price')
plt.show()

# Shapiro-Wilk Test for Normality
shapiro_test = stats.shapiro(df['Comparable Sale Price'])
print(f"Shapiro-Wilk Test for Normality (Price):\nStatistic={shapiro_test.statistic:.4f}, p-value={shapiro_test.pvalue:.4f}")
if shapiro_test.pvalue > 0.05:
    print("Distribution looks Normal (fail to reject H0)")
else:
    print("Distribution does NOT look Normal (reject H0)")

# Univariate Analysis: Other Features
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Lot Size Distribution
sns.histplot(df['LotSize Comparable'], kde=True, ax=axes[0, 0])
axes[0, 0].set_title('Lot Size Distribution')

# Year Built Distribution
sns.histplot(df['YearBuilt Comparable'], bins=10, kde=True, ax=axes[0, 1])
axes[0, 1].set_title('Year Built Distribution')

# Bedrooms Frequency
sns.countplot(x='Bedrooms', data=df, ax=axes[1, 0])
axes[1, 0].set_title('Bedrooms Frequency')

# Bathrooms Frequency
sns.countplot(x='Bathrooms Comparable', data=df, ax=axes[1, 1])
axes[1, 1].set_title('Bathrooms Frequency')

plt.tight_layout()
plt.show()

# Bivariate Relationships
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Price vs Lot Size
sns.scatterplot(x='LotSize Comparable', y='Comparable Sale Price', data=df, ax=axes[0, 0])
axes[0, 0].set_title('Price vs Lot Size')

# 2. Price vs Year Built
sns.scatterplot(x='YearBuilt Comparable', y='Comparable Sale Price', data=df, ax=axes[0, 1])
axes[0, 1].set_title('Price vs Year Built')

# 3. Price vs Bathrooms
sns.boxplot(x='Bathrooms Comparable', y='Comparable Sale Price', data=df, ax=axes[1, 0])
axes[1, 0].set_title('Price vs Bathrooms')

# 4. Lot Size vs Year Built
sns.scatterplot(x='YearBuilt Comparable', y='LotSize Comparable', data=df, ax=axes[1, 1])
axes[1, 1].set_title('Lot Size vs Year Built')

plt.tight_layout()
plt.show()

# Outlier Detection
continuous_cols = ['Comparable Sale Price', 'LotSize Comparable']

# Visual Boxplots
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[continuous_cols])
plt.title('Boxplots for Continuous Variables')
plt.show()

# Statistical Detection (Z-score & IQR)
print("Outlier Analysis:")
for col in continuous_cols:
    # Z-score method
    z_scores = np.abs(stats.zscore(df[col]))
    z_outliers = np.sum(z_scores > 3)

    # IQR method
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    iqr_outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()

    print(f"Feature '{col}':")
    print(f"  - Z-score outliers (|z| > 3): {z_outliers}")
    print(f"  - IQR outliers: {iqr_outliers}")

"""### 3. Feature Engineering"""

# 1. Duplicate Resolution
print(f"Shape before deduplication: {df.shape}")

# Drop duplicates based on zpid, keeping the first occurrence
df_clean = df.drop_duplicates(subset='Comparable zpid', keep='first').copy()

print(f"Shape after deduplication: {df_clean.shape}")

# 2. Correlation Analysis
plt.figure(figsize=(10, 8))

# Select numeric columns only
numeric_df = df_clean.select_dtypes(include=[np.number])
corr_matrix = numeric_df.corr()

# Plot Heatmap
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix of Numeric Features')
plt.show()

# Identify strongest correlation with price
price_corr = corr_matrix['Comparable Sale Price'].sort_values(ascending=False)
print("\nCorrelations with Price:")
print(price_corr)

# 3. Feature Engineering

# Home Age (assuming current year 2024)
df_clean['home_age'] = 2024 - df_clean['YearBuilt Comparable']

# Price per SqFt Lot
df_clean['price_per_sqft_lot'] = df_clean['Comparable Sale Price'] / df_clean['LotSize Comparable']

# Lot Size Normalization
# Z-score
df_clean['lotsize_zscore'] = stats.zscore(df_clean['LotSize Comparable'])
# Min-Max
df_clean['lotsize_minmax'] = (df_clean['LotSize Comparable'] - df_clean['LotSize Comparable'].min()) / (df_clean['LotSize Comparable'].max() - df_clean['LotSize Comparable'].min())

# Construction Era
def categorize_era(year):
    if year < 1945:
        return 'Pre-War'
    elif 1945 <= year <= 1955:
        return 'Post-War'
    else:
        return 'Mid-Century'

df_clean['construction_era'] = df_clean['YearBuilt Comparable'].apply(categorize_era)

# Has Extra Bath (Binary indicator)
df_clean['has_extra_bath'] = (df_clean['Bathrooms Comparable'] >= 2).astype(int)

# Age Category (Binning)
# Using quartiles to define 'Historic', 'Vintage', 'Mid-Age', 'Modern' based on distribution
df_clean['age_category'] = pd.qcut(df_clean['YearBuilt Comparable'], q=4, labels=['Historic', 'Vintage', 'Mid-Age', 'Modern'])

print("New Features Created:")
display(df_clean[['home_age', 'price_per_sqft_lot', 'construction_era', 'has_extra_bath', 'age_category']].head())

# 4. Stratified Subset Creation

# Price Quartiles
df_clean['price_quartile'] = pd.qcut(df_clean['Comparable Sale Price'], q=4, labels=['Low', 'Mid-Low', 'Mid-High', 'High'])

print("Price Quartile Distribution:")
print(df_clean['price_quartile'].value_counts())

# Similar Homes Clusters (Group by Bathrooms and Age Category)
clusters = df_clean.groupby(['Bathrooms Comparable', 'age_category'], observed=False).size()
print("\nSample Clusters (Bathrooms, Age Category):")
print(clusters[clusters > 0])

# Edge Cases (Outliers based on Price or Lot Size Z-score > 2.5 for demonstration)
edge_cases = df_clean[
    (np.abs(stats.zscore(df_clean['Comparable Sale Price'])) > 2.5) |
    (np.abs(stats.zscore(df_clean['LotSize Comparable'])) > 2.5)
]
print(f"\nIdentified {len(edge_cases)} potential edge cases for stress testing.")

import json

# 5. Statistical Summary Report

# Collect statistics
stats_dict = {
    "dataset_metadata": {
        "original_shape": df.shape,
        "cleaned_shape": df_clean.shape,
        "duplicates_removed": df.shape[0] - df_clean.shape[0]
    },
    "feature_statistics": {
        "skewness": df_clean.select_dtypes(include=[np.number]).skew().to_dict(),
        "kurtosis": df_clean.select_dtypes(include=[np.number]).kurtosis().to_dict()
    },
    "correlation_with_price": df_clean.select_dtypes(include=[np.number]).corr()['Comparable Sale Price'].to_dict(),
    "outlier_analysis": {
        "edge_cases_identified": len(edge_cases)
    },
    "transformations_log": [
        "Deduplication (kept first)",
        "Feature Engineering: home_age, price_per_sqft_lot, lotsize_zscore, construction_era",
        "Categorization: age_category, price_quartile"
    ]
}

# Helper to handle numpy types for JSON
def convert_numpy(obj):
    if isinstance(obj, np.integer): return int(obj)
    if isinstance(obj, np.floating): return float(obj)
    if isinstance(obj, np.ndarray): return obj.tolist()
    return obj

# Save to JSON
with open('preprocessing_report.json', 'w') as f:
    json.dump(stats_dict, f, default=convert_numpy, indent=4)

print("preprocessing_report.json generated successfully.")
print("\nJSON Report Content:")
print(json.dumps(stats_dict, default=convert_numpy, indent=4))

# 6. Final Visualizations

# Scatter Matrix colored by Bathroom count
sns.pairplot(df_clean,
             vars=['Comparable Sale Price', 'LotSize Comparable', 'YearBuilt Comparable', 'home_age'],
             hue='Bathrooms Comparable',
             palette='viridis')
plt.suptitle("Scatter Matrix by Bathroom Count", y=1.02)
plt.show()

# Temporal Price Trends
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_clean, x='YearBuilt Comparable', y='Comparable Sale Price', hue='construction_era', s=100)
sns.regplot(data=df_clean, x='YearBuilt Comparable', y='Comparable Sale Price', scatter=False, color='gray', line_kws={'linestyle': '--', 'alpha': 0.5})
plt.title("Temporal Price Trends: Price vs Year Built")
plt.grid(True, alpha=0.3)
plt.show()

# Stratification Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Price distribution by Quartile
sns.boxplot(data=df_clean, x='price_quartile', y='Comparable Sale Price', ax=axes[0])
axes[0].set_title("Price Distribution by Price Quartile")

# Feature distribution across quartiles (e.g., Lot Size)
sns.boxplot(data=df_clean, x='price_quartile', y='LotSize Comparable', ax=axes[1])
axes[1].set_title("Lot Size Distribution by Price Quartile")

plt.tight_layout()
plt.show()

# 7. Data Export

# Full Processed Dataset
df_clean.to_csv('homes_processed.csv', index=False)
print("Preview of homes_processed.csv:")
display(df_clean.head())

# Balanced Stratified Sample (Taking up to 3 samples per quartile for a mini-batch)
balanced_sample = df_clean.groupby('price_quartile', observed=False, group_keys=False).apply(lambda x: x.sample(min(len(x), 3), random_state=42))
balanced_sample.to_csv('homes_stratified.csv', index=False)
print("\nPreview of homes_stratified.csv:")
display(balanced_sample.head())

# Edge Cases
if not edge_cases.empty:
    edge_cases.to_csv('homes_edge_cases.csv', index=False)
    print("\nPreview of homes_edge_cases.csv:")
    display(edge_cases.head())
else:
    # Create an empty file or a placeholder if no edge cases found
    empty_df = pd.DataFrame(columns=df_clean.columns)
    empty_df.to_csv('homes_edge_cases.csv', index=False)
    print("\nPreview of homes_edge_cases.csv (Empty):")
    display(empty_df.head())

print("\nExport Complete:")
print("- homes_processed.csv")
print("- homes_stratified.csv")
print("- homes_edge_cases.csv")

"""### 4. API Setup & Configuration"""

!pip install anthropic mistralai

import os
import sys
from google.colab import userdata

def ping_llms():
    print("Checking LLM Connectivity & Environment Setup...\n")

    # 1. Check Claude (Anthropic)
    print("--- Checking Claude (Anthropic) ---")
    try:
        import anthropic
        print("‚úÖ 'anthropic' library installed.")

        # Load API Key from Secrets
        try:
            claude_key = userdata.get('ANTHROPIC_API_KEY')
        except:
            try:
                claude_key = userdata.get('Claude')
            except:
                claude_key = None

        if claude_key:
            os.environ['ANTHROPIC_API_KEY'] = claude_key
            print("‚úÖ ANTHROPIC_API_KEY loaded from Secrets.")

            # Test Connection
            try:
                print("‚è≥ Pinging Claude...")
                client = anthropic.Anthropic()
                message = client.messages.create(
                    model="claude-sonnet-4-5",
                    max_tokens=1000,
                    messages=[
                        {"role": "user", "content": "Explain NLP in 50 words ? "}
                    ]
                )
                print(f"üí¨ Claude Response: {message.content[0].text}")
                print("‚úÖ Claude is working.")
            except Exception as e:
                print(f"‚ùå Claude Test Failed: {e}")

        elif "ANTHROPIC_API_KEY" in os.environ:
            print("‚úÖ ANTHROPIC_API_KEY found in environment.")
        else:
            print("‚ö†Ô∏è ANTHROPIC_API_KEY NOT found.")

    except ImportError:
        print("‚ùå 'anthropic' library NOT installed.")

    print("\n--- Checking Mistral ---")
    try:
        from mistralai import Mistral
        print("‚úÖ 'mistralai' library installed.")

        # Load API Key from Secrets
        try:
            mistral_key = userdata.get('MISTRAL_API_KEY')
        except:
            try:
                mistral_key = userdata.get('Mistral')
            except:
                mistral_key = None

        if mistral_key:
            os.environ['MISTRAL_API_KEY'] = mistral_key
            print("‚úÖ MISTRAL_API_KEY loaded from Secrets.")

            # Test Connection
            try:
                print("‚è≥ Pinging Mistral...")
                with Mistral(api_key=mistral_key) as mistral:
                    res = mistral.chat.complete(
                        model="mistral-large-2411",
                        messages=[
                            {"role": "user", "content": "Explain NLP in 75 words ?"}
                        ],
                        stream=False
                    )
                    # Mistral SDK v1+ response structure
                    content = res.choices[0].message.content
                    print(f"üí¨ Mistral Response: {content}")
                    print("‚úÖ Mistral is working.")
            except Exception as e:
                print(f"‚ùå Mistral Test Failed: {e}")

        elif "MISTRAL_API_KEY" in os.environ:
            print("‚úÖ MISTRAL_API_KEY found in environment.")
        else:
            print("‚ö†Ô∏è MISTRAL_API_KEY NOT found.")

    except ImportError:
        print("‚ùå 'mistralai' library NOT installed or version mismatch.")

ping_llms()

import requests
import json
from google.colab import userdata

def ping_openrouter():
    print("\n--- Checking OpenRouter (Llama 3.3) ---")
    try:
        # Load API Key from Secrets
        try:
            openrouter_key = userdata.get('Llama')
        except:
            try:
                openrouter_key = userdata.get('OpenRouter')
            except:
                openrouter_key = None

        if openrouter_key:
            print("‚úÖ OPENROUTER_API_KEY loaded from Secrets.")

            print("‚è≥ Pinging OpenRouter (Llama 3.3)...")
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {openrouter_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://colab.research.google.com/",
                    "X-Title": "Colab Agent Test",
                },
                data=json.dumps({
                    "model": "meta-llama/llama-3.3-70b-instruct:free",
                    "messages": [
                        {
                            "role": "user",
                            "content": "Explain NLP in 50 words?"
                        }
                    ]
                })
            )

            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content']
                    print(f"üí¨ OpenRouter Response: {content}")
                    print("‚úÖ OpenRouter (Llama 3.3) is working.")
                else:
                    print(f"‚ö†Ô∏è Unexpected response structure: {result}")
            else:
                print(f"‚ùå OpenRouter Request Failed: {response.status_code} - {response.text}")
        else:
            print("‚ö†Ô∏è OPENROUTER_API_KEY NOT found in Secrets.")

    except Exception as e:
        print(f"‚ùå OpenRouter Test Failed: {e}")

ping_openrouter()

"""### 4. Observability Integration"""

!pip install weave
import weave
weave.init("llm-home-valuation-study")

"""### 5. Prompt Engineering, LLM Evaluation Framework & Test Run"""

# Prompt Strategies

PROMPT_STRATEGIES = {
    "zero_shot": """Estimate the market value of a residential property with the following characteristics:
- Bedrooms: {bedrooms}
- Bathrooms: {bathrooms}
- Lot Size: {lot_size} square feet
- Year Built: {year_built}

Provide a single estimated value in dollars. Format your response as: "Estimated value: $XXX,XXX" """,

    "few_shot": """You are estimating the market value of residential properties. Here are three comparable home sales:

Comparable 1: 3 bedrooms, {comp1_baths} bathrooms, {comp1_lot} sq ft lot, built {comp1_year} - Sold for ${comp1_price:,}
Comparable 2: 3 bedrooms, {comp2_baths} bathrooms, {comp2_lot} sq ft lot, built {comp2_year} - Sold for ${comp2_price:,}
Comparable 3: 3 bedrooms, {comp3_baths} bathrooms, {comp3_lot} sq ft lot, built {comp3_year} - Sold for ${comp3_price:,}

Based on these comparables, estimate the value of this property:
- Bedrooms: {bedrooms}
- Bathrooms: {bathrooms}
- Lot Size: {lot_size} square feet
- Year Built: {year_built}

Provide a single estimated value in dollars. Format your response as: "Estimated value: $XXX,XXX" """,

    "chain_of_thought": """Estimate the market value of this residential property using step-by-step reasoning:

Property Details:
- Bedrooms: {bedrooms}
- Bathrooms: {bathrooms}
- Lot Size: {lot_size} square feet
- Year Built: {year_built}

Break down your valuation into these components:
1. Base lot value (cost per square foot for land)
2. Structure value (replacement cost considering bedrooms and bathrooms)
3. Age depreciation or historic premium
4. Final estimated market value

Show your reasoning for each step, then provide your final estimate. Format your final answer as: "Final estimated value: $XXX,XXX" """,

    "role_playing": """You are a certified residential real estate appraiser with 20 years of experience specializing in mid-20th century homes. You have completed over 5,000 appraisals and understand market dynamics for vintage properties.

A client has requested your professional valuation for this property:
- Bedrooms: {bedrooms}
- Bathrooms: {bathrooms}
- Lot Size: {lot_size} square feet
- Year Built: {year_built}

Provide your professional appraisal estimate as a single dollar amount. Format your response as: "Professional estimate: $XXX,XXX" """,

    "constraint_based": """Estimate the market value of this residential property in a mid-20th century urban neighborhood where similar homes typically sell between $100,000 and $500,000:

Property Details:
- Bedrooms: {bedrooms}
- Bathrooms: {bathrooms}
- Lot Size: {lot_size} square feet
- Year Built: {year_built}

Neighborhood Context:
- Historic character and walkability are highly valued
- Homes from the 1925-1965 era are particularly sought after
- Typical lot sizes range from 3,000 to 9,400 square feet
- Properties with 2-3 bathrooms command premium prices
- Larger lots (7,000+ sq ft) add significant value

Based on these neighborhood characteristics and the property details, provide a single estimated value. Format your response as: "Estimated value: $XXX,XXX" """
}


def format_prompt_for_valuation(strategy_name, home_data, comparables_data=None):
    """
    Format a prompt template with home property data.

    Args:
        strategy_name: One of 'zero_shot', 'few_shot', 'chain_of_thought', 'role_playing', 'constraint_based'
        home_data: Dict with keys: bedrooms, bathrooms, lot_size, year_built
        comparables_data: List of 3 dicts for few_shot (each with bathrooms, lot_size, year_built, price)

    Returns:
        Formatted prompt string
    """
    if strategy_name not in PROMPT_STRATEGIES:
        raise ValueError(f"Unknown strategy: {strategy_name}. Choose from {list(PROMPT_STRATEGIES.keys())}")

    template = PROMPT_STRATEGIES[strategy_name]

    if strategy_name == "few_shot":
        if not comparables_data or len(comparables_data) != 3:
            raise ValueError("few_shot strategy requires exactly 3 comparables")

        prompt_data = {
            **home_data,
            'comp1_baths': comparables_data[0]['bathrooms'],
            'comp1_lot': comparables_data[0]['lot_size'],
            'comp1_year': comparables_data[0]['year_built'],
            'comp1_price': comparables_data[0]['price'],
            'comp2_baths': comparables_data[1]['bathrooms'],
            'comp2_lot': comparables_data[1]['lot_size'],
            'comp2_year': comparables_data[1]['year_built'],
            'comp2_price': comparables_data[1]['price'],
            'comp3_baths': comparables_data[2]['bathrooms'],
            'comp3_lot': comparables_data[2]['lot_size'],
            'comp3_year': comparables_data[2]['year_built'],
            'comp3_price': comparables_data[2]['price'],
        }
        return template.format(**prompt_data)
    else:
        return template.format(**home_data)


def get_all_strategy_names():
    """Return list of all available prompt strategy names."""
    return list(PROMPT_STRATEGIES.keys())

# LLM Calling and Evaluation

import re
import time
import anthropic
from mistralai import Mistral
import requests
import weave


def extract_price_from_response(text):
    """
    Extract dollar amount from LLM response using multiple regex patterns.
    Returns integer value or None if extraction fails.
    """
    patterns = [
        r'(?:estimated?|final|professional)\s+(?:value|estimate):\s*\$?\s*(\d{1,3}(?:,\d{3})*)',
        r'(?:value|estimate|price)(?:\s+is|\s+of)?\s*:?\s*\$\s*(\d{1,3}(?:,\d{3})*)',
        r'\$\s*(\d{1,3}(?:,\d{3})*)\s*(?:dollars?)?(?:\s|$|\.|,)',
        r'(?:approximately|around|about)\s+\$\s*(\d{1,3}(?:,\d{3})*)'
    ]

    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            for match in matches:
                price_str = match.replace(',', '').replace(' ', '')
                try:
                    value = int(float(price_str))
                    if 50000 <= value <= 1000000:
                        return value
                except:
                    continue

    all_numbers = re.findall(r'\$?\s*(\d{1,3}(?:,\d{3})+)', text)
    for num in all_numbers:
        try:
            value = int(num.replace(',', ''))
            if 100000 <= value <= 400000:
                return value
        except:
            continue

    return None


@weave.op()
def call_claude_valuation(prompt, api_key):
    """Call Claude API for home valuation."""
    client = anthropic.Anthropic(api_key=api_key)
    message = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=2000,
        temperature=0.3,
        messages=[{"role": "user", "content": prompt}]
    )
    return {
        'response': message.content[0].text,
        'tokens': message.usage.input_tokens + message.usage.output_tokens
    }


@weave.op()
def call_mistral_valuation(prompt, api_key):
    """Call Mistral API for home valuation."""
    with Mistral(api_key=api_key) as client:
        response = client.chat.complete(
            model="mistral-large-2411",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=2000
        )
    return {
        'response': response.choices[0].message.content,
        'tokens': response.usage.total_tokens if hasattr(response, 'usage') else None
    }


@weave.op()
def call_llama_valuation(prompt, api_key):
    """Call Llama via OpenRouter for home valuation."""
    response = requests.post(
        url="https://openrouter.ai/api/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        },
        json={
            "model": "meta-llama/llama-3.3-70b-instruct:free",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 2000
        }
    )
    data = response.json()
    return {
        'response': data['choices'][0]['message']['content'],
        'tokens': data.get('usage', {}).get('total_tokens')
    }

@weave.op()
def call_llama_valuation_aws(prompt, api_key):
    """Call Llama 3.3 70B via AWS Bedrock Converse API."""
    import boto3
    from google.colab import userdata

    client = boto3.client(
        'bedrock-runtime',
        region_name='us-east-1',
        aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),
        aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY')
    )

    response = client.converse(
        modelId='us.meta.llama3-3-70b-instruct-v1:0',
        messages=[{"role": "user", "content": [{"text": prompt}]}],
        inferenceConfig={
            "maxTokens": 512,
            "temperature": 0.3
        }
    )

    return {
        'response': response['output']['message']['content'][0]['text'],
        'tokens': response['usage']['inputTokens'] + response['usage']['outputTokens']
    }

@weave.op()
def call_mistral_valuation_aws(prompt, api_key):
    """Call Mistral Large via AWS Bedrock Converse API."""
    import boto3
    from google.colab import userdata

    client = boto3.client(
        'bedrock-runtime',
        region_name='us-east-1',
        aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),
        aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY')
    )

    response = client.converse(
        modelId='mistral.mistral-large-2402-v1:0',
        messages=[{"role": "user", "content": [{"text": prompt}]}],
        inferenceConfig={
            "maxTokens": 2000,
            "temperature": 0.3
        }
    )

    return {
        'response': response['output']['message']['content'][0]['text'],
        'tokens': response['usage']['inputTokens'] + response['usage']['outputTokens']
    }

@weave.op()
def evaluate_llm_home_valuation(model_name, strategy_name, prompt, api_keys, actual_price):
    """
    Call LLM for home valuation and evaluate results.

    Args:
        model_name: 'claude', 'mistral', 'mistral_aws', 'llama', or 'llama_aws'
        strategy_name: Name of prompt strategy used
        prompt: Formatted prompt string
        api_keys: Dict with keys 'claude', 'mistral', 'llama'
        actual_price: Ground truth price for evaluation
    """
    start_time = time.time()

    try:
        if model_name == 'claude':
            result = call_claude_valuation(prompt, api_keys['claude'])
        elif model_name == 'mistral':
            result = call_mistral_valuation(prompt, api_keys['mistral'])
        elif model_name == 'mistral_aws':
            result = call_mistral_valuation_aws(prompt, None)
        elif model_name == 'llama':
            result = call_llama_valuation(prompt, api_keys['llama'])
        elif model_name == 'llama_aws':
            result = call_llama_valuation_aws(prompt, api_keys['llama'])
        else:
            raise ValueError(f"Unknown model: {model_name}")

        estimated_value = extract_price_from_response(result['response'])
        latency = time.time() - start_time

        evaluation_result = {
            'model': model_name,
            'strategy': strategy_name,
            'response': result['response'],
            'estimated_value': estimated_value,
            'actual_price': actual_price,
            'tokens': result['tokens'],
            'latency': latency,
            'success': estimated_value is not None
        }

        if estimated_value:
            error = estimated_value - actual_price
            evaluation_result['error'] = error
            evaluation_result['absolute_error'] = abs(error)
            evaluation_result['percent_error'] = (error / actual_price) * 100
            evaluation_result['absolute_percent_error'] = abs(evaluation_result['percent_error'])

        return evaluation_result

    except Exception as e:
        return {
            'model': model_name,
            'strategy': strategy_name,
            'response': None,
            'estimated_value': None,
            'actual_price': actual_price,
            'tokens': None,
            'latency': time.time() - start_time,
            'success': False,
            'error_message': str(e)
        }


def batch_evaluate_home_valuations(home_data, actual_price, strategies, models, api_keys, comparables_data=None, delay=1):
    """
    Run batch evaluation across multiple strategies and models.

    Args:
        home_data: Dict with property details
        actual_price: Ground truth price
        strategies: List of strategy names to test
        models: List of model names to test
        api_keys: Dict with API keys
        comparables_data: List of 3 comparables for few_shot
        delay: Sleep delay between API calls

    Returns:
        List of evaluation result dicts
    """
    #from prompts import format_prompt_for_valuation

    results = []

    for strategy in strategies:
        if strategy == 'few_shot':
            prompt = format_prompt_for_valuation(strategy, home_data, comparables_data)
        else:
            prompt = format_prompt_for_valuation(strategy, home_data)

        for model in models:
            result = evaluate_llm_home_valuation(model, strategy, prompt, api_keys, actual_price)
            results.append(result)
            time.sleep(delay)

    return results

import pandas as pd
import weave
from google.colab import userdata

# Initialize Weave tracking
weave.init("llm-home-valuation-study-test-run")

# Load stratified dataset
df_stratified = pd.read_csv('homes_stratified.csv')

# Select first property as test case
test_row = df_stratified.iloc[0]
test_home = {
    'bedrooms': int(test_row['Bedrooms']),
    'bathrooms': int(test_row['Bathrooms Comparable']),
    'lot_size': int(test_row['LotSize Comparable']),
    'year_built': int(test_row['YearBuilt Comparable'])
}
actual_price = int(test_row['Comparable Sale Price'])

# Select 3 comparables from other properties
comp_rows = df_stratified.iloc[1:4]
comparables = [
    {
        'bathrooms': int(row['Bathrooms Comparable']),
        'lot_size': int(row['LotSize Comparable']),
        'year_built': int(row['YearBuilt Comparable']),
        'price': int(row['Comparable Sale Price'])
    }
    for _, row in comp_rows.iterrows()
]

# API keys from Colab secrets
api_keys = {
    'claude': userdata.get('Claude'),
    'mistral': userdata.get('Mistral'),
    'llama': userdata.get('Llama')
}

# Models to test
models = ['claude', 'mistral', 'llama']
strategies = get_all_strategy_names()

print("="*80)
print("HOME VALUATION STUDY: 5 Strategies √ó 3 Models = 15 API Calls")
print("="*80)
print(f"\nTest Property: {test_home['bedrooms']}BR/{test_home['bathrooms']}BA, {test_home['lot_size']} sq ft, built {test_home['year_built']}")
print(f"Actual Price: ${actual_price:,}\n")

# Run batch evaluation
results = batch_evaluate_home_valuations(
    home_data=test_home,
    actual_price=actual_price,
    strategies=strategies,
    models=models,
    api_keys=api_keys,
    comparables_data=comparables,
    delay=1
)

# Display results
for strategy in strategies:
    print(f"\nüìù {strategy.upper().replace('_', ' ')}")
    for result in [r for r in results if r['strategy'] == strategy]:
        if result['success']:
            print(f"  {result['model']:<8}: ${result['estimated_value']:,} ({result['percent_error']:+.1f}%)")
        else:
            print(f"  {result['model']:<8}: ‚ö†Ô∏è Extraction Failed")

# Convert to DataFrame
results_df = pd.DataFrame(results)

# Create comparison table
print("\n" + "="*80)
print("COMPARISON TABLE")
print("="*80)
comparison = results_df[results_df['success']].pivot_table(
    values='estimated_value',
    index='strategy',
    columns='model',
    aggfunc='first'
)
comparison['actual'] = actual_price
print(comparison)

# Summary statistics
print("\n" + "="*80)
print("SUMMARY STATISTICS")
print("="*80)
valid_results = results_df[results_df['success']]
print(f"Successful extractions: {len(valid_results)}/{len(results)}")
print(f"Mean estimate: ${valid_results['estimated_value'].mean():,.0f}")
print(f"Std deviation: ${valid_results['estimated_value'].std():,.0f}")
print(f"Range: ${valid_results['estimated_value'].min():,.0f} - ${valid_results['estimated_value'].max():,.0f}")
print(f"Mean absolute % error: {valid_results['absolute_percent_error'].mean():.1f}%")
print(f"Median absolute % error: {valid_results['absolute_percent_error'].median():.1f}%")

# Performance by model
print("\n" + "="*80)
print("PERFORMANCE BY MODEL")
print("="*80)
model_performance = valid_results.groupby('model').agg({
    'absolute_percent_error': ['mean', 'median'],
    'latency': 'mean',
    'tokens': 'mean'
}).round(2)
print(model_performance)

# Performance by strategy
print("\n" + "="*80)
print("PERFORMANCE BY STRATEGY")
print("="*80)
strategy_performance = valid_results.groupby('strategy').agg({
    'absolute_percent_error': ['mean', 'median'],
    'estimated_value': ['mean', 'std']
}).round(2)
print(strategy_performance)

# Save results
results_df.to_csv('home_valuation_test_results.csv', index=False)
print("\n‚úÖ Results saved to home_valuation_test_results.csv")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load results
results_df = pd.read_csv('home_valuation_test_results.csv')

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (48, 36)

# Create comprehensive visualization
fig = plt.figure(figsize=(24, 18))
gs = fig.add_gridspec(4, 3, hspace=1, wspace=1)

# 1. Estimated Values by Strategy and Model
ax1 = fig.add_subplot(gs[0, :])
pivot_estimates = results_df[results_df['success']].pivot(index='strategy', columns='model', values='estimated_value')
pivot_estimates.plot(kind='bar', ax=ax1, width=0.8)
ax1.axhline(y=160000, color='red', linestyle='--', linewidth=2, label='Actual Price')
ax1.set_title('Estimated Values by Strategy and Model', fontsize=14, fontweight='bold')
ax1.set_ylabel('Estimated Value ($)')
ax1.set_xlabel('Strategy')
ax1.legend(title='Model')
ax1.tick_params(axis='x', rotation=45)

# 2. Absolute Percent Error by Strategy
ax2 = fig.add_subplot(gs[1, 0])
strategy_errors = results_df[results_df['success']].groupby('strategy')['absolute_percent_error'].mean().sort_values()
strategy_errors.plot(kind='barh', ax=ax2, color='coral')
ax2.set_title('Mean Absolute % Error by Strategy', fontsize=12, fontweight='bold')
ax2.set_xlabel('Absolute Percent Error (%)')

# 3. Absolute Percent Error by Model
ax3 = fig.add_subplot(gs[1, 1])
model_errors = results_df[results_df['success']].groupby('model')['absolute_percent_error'].mean().sort_values()
model_errors.plot(kind='barh', ax=ax3, color='skyblue')
ax3.set_title('Mean Absolute % Error by Model', fontsize=12, fontweight='bold')
ax3.set_xlabel('Absolute Percent Error (%)')

# 4. Token Usage by Model
ax4 = fig.add_subplot(gs[1, 2])
token_usage = results_df[results_df['success']].groupby('model')['tokens'].mean()
token_usage.plot(kind='bar', ax=ax4, color='lightgreen')
ax4.set_title('Average Token Usage by Model', fontsize=12, fontweight='bold')
ax4.set_ylabel('Tokens')
ax4.tick_params(axis='x', rotation=0)

# 5. Latency Comparison
ax5 = fig.add_subplot(gs[2, 0])
latency_data = results_df[results_df['success']].groupby('model')['latency'].mean()
latency_data.plot(kind='bar', ax=ax5, color='plum')
ax5.set_title('Average Latency by Model', fontsize=12, fontweight='bold')
ax5.set_ylabel('Latency (seconds)')
ax5.tick_params(axis='x', rotation=0)

# 6. Error Distribution Boxplot
ax6 = fig.add_subplot(gs[2, 1:])
results_df[results_df['success']].boxplot(column='absolute_percent_error', by='strategy', ax=ax6)
ax6.set_title('Error Distribution by Strategy', fontsize=12, fontweight='bold')
ax6.set_ylabel('Absolute Percent Error (%)')
ax6.set_xlabel('Strategy')
plt.suptitle('')  # Remove default title

# 7. Heatmap: Estimated Values
ax7 = fig.add_subplot(gs[3, :2])
heatmap_data = results_df[results_df['success']].pivot(index='strategy', columns='model', values='estimated_value')
sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax7, cbar_kws={'label': 'Estimated Value ($)'})
ax7.set_title('Heatmap: Estimated Values by Strategy √ó Model', fontsize=12, fontweight='bold')

# 8. Success Rate
ax8 = fig.add_subplot(gs[3, 2])
success_by_model = results_df.groupby('model')['success'].apply(lambda x: (x.sum() / len(x)) * 100)
success_by_model.plot(kind='bar', ax=ax8, color='mediumseagreen')
ax8.set_title('Success Rate by Model', fontsize=12, fontweight='bold')
ax8.set_ylabel('Success Rate (%)')
ax8.set_ylim([0, 105])
ax8.tick_params(axis='x', rotation=0)
for i, v in enumerate(success_by_model):
    ax8.text(i, v + 2, f'{v:.0f}%', ha='center', fontweight='bold')

plt.tight_layout()
plt.savefig('valuation_analysis_comprehensive.png', dpi=300, bbox_inches='tight')
plt.show()

# Failed Extraction Analysis
print("\n" + "="*80)
print("FAILED EXTRACTION ANALYSIS")
print("="*80)

failed_results = results_df[~results_df['success']]
if len(failed_results) > 0:
    for idx, row in failed_results.iterrows():
        print(f"\nModel: {row['model']}")
        print(f"Strategy: {row['strategy']}")
        print(f"Success: {row['success']}")
        print(f"\nFull Response:")
        print("-" * 80)
        print(row['response'])
        print("-" * 80)
        print("\nIssue: Price extraction regex patterns failed to parse this response format.")
        print("The model likely provided narrative explanation without clear dollar formatting.")
else:
    print("\nNo failed extractions found.")

# Detailed Statistics Table
print("\n" + "="*80)
print("DETAILED PERFORMANCE METRICS")
print("="*80)

detailed_stats = results_df[results_df['success']].groupby(['strategy', 'model']).agg({
    'estimated_value': 'first',
    'absolute_percent_error': 'first',
    'tokens': 'first',
    'latency': 'first'
}).round(2)

print(detailed_stats)

# Strategy Consistency Analysis
print("\n" + "="*80)
print("STRATEGY CONSISTENCY (Standard Deviation of Estimates)")
print("="*80)

consistency = results_df[results_df['success']].groupby('strategy')['estimated_value'].agg(['mean', 'std']).round(0)
consistency['cv'] = (consistency['std'] / consistency['mean'] * 100).round(2)
consistency.columns = ['Mean Estimate ($)', 'Std Dev ($)', 'Coefficient of Variation (%)']
print(consistency.sort_values('Std Dev ($)'))

print("\nLower CV = More consistent across models")

"""### 6. Full Run & Analysis"""

!pip install boto3 -q
import boto3

!pip install boto3 -q

import boto3
import json
from google.colab import userdata

# Create Bedrock client
client = boto3.client(
    'bedrock-runtime',
    region_name='us-east-1',
    aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY')
)

# Test prompt
test_prompt = "What is 2+2? Answer in one word."

# Call Llama 3.3 via Converse API - USE CROSS-REGION INFERENCE PROFILE
response = client.converse(
    modelId='us.meta.llama3-3-70b-instruct-v1:0',  # Added 'us.' prefix
    messages=[{"role": "user", "content": [{"text": test_prompt}]}],
    inferenceConfig={
        "maxTokens": 100,
        "temperature": 0.3
    }
)

# Print results
print("‚úÖ Bedrock connection successful!\n")
print(f"Response: {response['output']['message']['content'][0]['text']}")
print(f"Input tokens: {response['usage']['inputTokens']}")
print(f"Output tokens: {response['usage']['outputTokens']}")
print(f"Total tokens: {response['usage']['inputTokens'] + response['usage']['outputTokens']}")

import boto3
import json
from google.colab import userdata

# Create Bedrock client
client = boto3.client(
    'bedrock-runtime',
    region_name='us-east-1',
    aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY')
)

# Test prompt
test_prompt = "What is 2+2? Answer in one word."

# Call Mistral Large (2402 version) via Converse API
response = client.converse(
    modelId='mistral.mistral-large-2402-v1:0',
    messages=[{"role": "user", "content": [{"text": test_prompt}]}],
    inferenceConfig={
        "maxTokens": 100,
        "temperature": 0.3
    }
)

# Print results
print("‚úÖ Mistral Bedrock connection successful!\n")
print(f"Response: {response['output']['message']['content'][0]['text']}")
print(f"Input tokens: {response['usage']['inputTokens']}")
print(f"Output tokens: {response['usage']['outputTokens']}")
print(f"Total tokens: {response['usage']['inputTokens'] + response['usage']['outputTokens']}")

import pandas as pd
import numpy as np
import weave
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from google.colab import userdata
from datetime import datetime
import boto3

# Initialize Weave tracking
weave.init("llm-home-valuation-study-full-run")

# Load datasets
print("Loading datasets...")
df_processed = pd.read_csv('homes_processed.csv')
df_stratified = pd.read_csv('homes_stratified.csv')

print(f"Loaded {len(df_processed)} properties from homes_processed.csv")
print(f"Loaded {len(df_stratified)} properties for comparables selection\n")

# API keys from Colab secrets
api_keys = {
    'claude': userdata.get('Claude'),
    'mistral': userdata.get('Mistral'),
    'llama': userdata.get('Llama')
}

# Models and strategies
models = ['claude', 'mistral_aws', 'llama_aws']
strategies = get_all_strategy_names()

# Delay between strategy batches (seconds)
BATCH_DELAY = 15  # Wait 15 seconds between each batch of 3 parallel calls

# Storage for all results
all_results = []
failed_properties = []

def evaluate_single_model(model, strategy, prompt, api_keys, actual_price, zpid, idx):
    """Worker function to evaluate a single model."""
    try:
        result = evaluate_llm_home_valuation(
            model_name=model,
            strategy_name=strategy,
            prompt=prompt,
            api_keys=api_keys,
            actual_price=actual_price
        )
        result['zpid'] = zpid
        result['property_index'] = idx
        return result
    except Exception as e:
        return {
            'zpid': zpid,
            'property_index': idx,
            'model': model,
            'strategy': strategy,
            'success': False,
            'error_message': str(e),
            'actual_price': actual_price
        }

# Start time
start_time = datetime.now()
print(f"Starting full evaluation at {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)
print(f"FULL-SCALE EVALUATION (PARALLEL): {len(df_processed)} Properties √ó 5 Strategies √ó 3 Models = {len(df_processed) * 15} API Calls")
print("="*80)
print(f"\nParallel Mode: 3 models hit simultaneously per strategy")
print(f"Batch Delay: {BATCH_DELAY}s between each strategy batch")
print(f"Estimated time per property: ~{len(strategies) * BATCH_DELAY}s ({len(strategies)} strategies √ó {BATCH_DELAY}s)")
print("\n" + "="*80 + "\n")

# Iterate through all properties
for idx, row in df_processed.iterrows():
    property_num = idx + 1
    print(f"\n{'='*80}")
    print(f"Processing Property {property_num}/{len(df_processed)}")
    print(f"{'='*80}")

    # Prepare home data
    test_home = {
        'bedrooms': int(row['Bedrooms']),
        'bathrooms': int(row['Bathrooms Comparable']),
        'lot_size': int(row['LotSize Comparable']),
        'year_built': int(row['YearBuilt Comparable'])
    }
    actual_price = int(row['Comparable Sale Price'])
    zpid = int(row['Comparable zpid'])

    print(f"ZPID: {zpid}")
    print(f"Property: {test_home['bedrooms']}BR/{test_home['bathrooms']}BA, {test_home['lot_size']} sqft, built {test_home['year_built']}")
    print(f"Actual Price: ${actual_price:,}\n")

    # Select 3 random comparables from stratified dataset (excluding current property if present)
    available_comps = df_stratified[df_stratified['Comparable zpid'] != zpid]
    if len(available_comps) >= 3:
        comp_rows = available_comps.sample(n=3, random_state=idx)
    else:
        comp_rows = df_stratified.iloc[:3]

    comparables = [
        {
            'bathrooms': int(comp_row['Bathrooms Comparable']),
            'lot_size': int(comp_row['LotSize Comparable']),
            'year_built': int(comp_row['YearBuilt Comparable']),
            'price': int(comp_row['Comparable Sale Price'])
        }
        for _, comp_row in comp_rows.iterrows()
    ]

    # Evaluate each strategy with parallel model calls
    property_results = []

    try:
        for strategy_idx, strategy in enumerate(strategies):
            print(f"\n  Strategy {strategy_idx + 1}/5: {strategy}")

            # Format prompt
            if strategy == 'few_shot':
                prompt = format_prompt_for_valuation(strategy, test_home, comparables)
            else:
                prompt = format_prompt_for_valuation(strategy, test_home)

            # Hit all 3 models in parallel
            batch_start = time.time()
            strategy_results = []

            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {
                    executor.submit(
                        evaluate_single_model,
                        model, strategy, prompt, api_keys, actual_price, zpid, idx
                    ): model for model in models
                }

                for future in as_completed(futures):
                    model = futures[future]
                    result = future.result()
                    strategy_results.append(result)

                    # Print result
                    if result.get('success'):
                        print(f"    ‚îî‚îÄ {model}: ‚úì ${result['estimated_value']:,} ({result['percent_error']:+.1f}%) [{result['latency']:.1f}s]")
                    else:
                        error_msg = result.get('error_message', 'Extraction failed')
                        latency = result.get('latency', 0)
                        print(f"    ‚îî‚îÄ {model}: ‚úó {error_msg[:50]} [{latency:.1f}s]")

            property_results.extend(strategy_results)

            # Wait before next strategy batch (except for last strategy of last property)
            is_last_strategy = (strategy_idx == len(strategies) - 1)
            is_last_property = (property_num == len(df_processed))

            if not (is_last_strategy and is_last_property):
                batch_duration = time.time() - batch_start
                wait_time = max(0, BATCH_DELAY - batch_duration)
                if wait_time > 0:
                    print(f"    ‚è≥ Waiting {wait_time:.1f}s before next batch...")
                    time.sleep(wait_time)

        # Add property results to all results
        all_results.extend(property_results)

        # Save incremental results every 5 properties
        if (property_num % 5 == 0) or (property_num == len(df_processed)):
            results_df_temp = pd.DataFrame(all_results)
            results_df_temp.to_csv('home_valuation_full_results_incremental.csv', index=False)
            print(f"\n  üíæ Incremental save: {len(all_results)} results saved")

        # Progress summary
        elapsed = (datetime.now() - start_time).total_seconds() / 60
        avg_time_per_property = elapsed / property_num
        remaining_properties = len(df_processed) - property_num
        estimated_remaining = avg_time_per_property * remaining_properties

        print(f"\n  ‚è±Ô∏è  Progress: {property_num}/{len(df_processed)} properties")
        print(f"      Elapsed: {elapsed:.1f} min | Est. remaining: {estimated_remaining:.1f} min")

    except Exception as e:
        print(f"\n  ‚ùå CRITICAL ERROR for property {zpid}: {str(e)}")
        failed_properties.append({'zpid': zpid, 'index': idx, 'error': str(e)})
        continue

# Final save
print("\n" + "="*80)
print("SAVING FINAL RESULTS")
print("="*80)

results_df = pd.DataFrame(all_results)
results_df.to_csv('home_valuation_full_results_FINAL.csv', index=False)

# Calculate summary statistics
end_time = datetime.now()
total_duration = (end_time - start_time).total_seconds() / 60

print(f"\n‚úÖ Evaluation Complete!")
print(f"   Total Duration: {total_duration:.1f} minutes")
print(f"   Total API Calls: {len(all_results)}")
print(f"   Failed Properties: {len(failed_properties)}")
print(f"\n   Results saved to: home_valuation_full_results_FINAL.csv")

# Success rate analysis
if len(results_df) > 0:
    successful = results_df[results_df['success'] == True]
    print(f"\n{'='*80}")
    print("OVERALL SUMMARY")
    print(f"{'='*80}")
    print(f"Successful Extractions: {len(successful)}/{len(results_df)} ({len(successful)/len(results_df)*100:.1f}%)")

    if len(successful) > 0:
        print(f"\nAccuracy Metrics:")
        print(f"  Mean Absolute % Error: {successful['absolute_percent_error'].mean():.2f}%")
        print(f"  Median Absolute % Error: {successful['absolute_percent_error'].median():.2f}%")
        print(f"  Mean Estimate: ${successful['estimated_value'].mean():,.0f}")
        print(f"  Std Dev: ${successful['estimated_value'].std():,.0f}")

        print(f"\nBy Model:")
        model_stats = successful.groupby('model').agg({
            'success': 'count',
            'absolute_percent_error': 'mean',
            'latency': 'mean',
            'tokens': 'mean'
        }).round(2)
        model_stats.columns = ['Count', 'Mean Error %', 'Avg Latency (s)', 'Avg Tokens']
        print(model_stats)

        print(f"\nBy Strategy:")
        strategy_stats = successful.groupby('strategy').agg({
            'success': 'count',
            'absolute_percent_error': 'mean',
            'estimated_value': ['mean', 'std']
        }).round(2)
        print(strategy_stats)

# Save failed properties log if any
if len(failed_properties) > 0:
    failed_df = pd.DataFrame(failed_properties)
    failed_df.to_csv('failed_properties_log.csv', index=False)
    print(f"\n‚ö†Ô∏è  Failed properties log saved to: failed_properties_log.csv")

print(f"\n{'='*80}")
print(f"View Weave dashboard at: https://wandb.ai/work-anshul95-carnegie-mellon-university/llm-home-valuation-study-full-run/weave")
print(f"{'='*80}")

# ==============================================================================
# LLM Home Valuation Study - Comprehensive Analysis Script
# ==============================================================================
# This script performs complete analysis of the home valuation experiment results
# including statistical tests, visualizations, and interpretability insights.
# ==============================================================================

# Cell 1: Install and Import Libraries
# ==============================================================================
print("Installing required packages...")
!pip install pandas matplotlib seaborn plotly scipy -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Configure visualization settings
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

print("‚úì Libraries loaded successfully")
print("="*80)

# Cell 2: Load and Validate Data
# ==============================================================================
print("\nLOADING DATA...")
print("="*80)

try:
    results_df = pd.read_csv('home_valuation_full_results_FINAL.csv')
    print(f"‚úì Results DataFrame loaded: {results_df.shape}")
    print(f"  Columns: {len(results_df.columns)}")
except FileNotFoundError:
    print("‚úó ERROR: 'home_valuation_full_results_FINAL.csv' not found")
    print("  Please upload the file and try again")
    results_df = pd.DataFrame()

try:
    weave_df = pd.read_csv('weave_export_llm-home-valuation-study-full-run_2025-11-29.csv')
    print(f"‚úì Weave DataFrame loaded: {weave_df.shape}")
    print(f"  Columns: {len(weave_df.columns)}")
except FileNotFoundError:
    print("‚ö† WARNING: Weave export file not found")
    print("  Cost analysis will be limited")
    weave_df = pd.DataFrame()

if results_df.empty:
    raise ValueError("Cannot proceed without results data. Please upload the CSV file.")

print("\nSample of loaded data:")
print(results_df.head(3))

# Cell 3: Data Preprocessing and Standardization
# ==============================================================================
print("\n" + "="*80)
print("DATA PREPROCESSING")
print("="*80)

# Detect format type and standardize columns
if 'model' in results_df.columns and 'output.model' not in results_df.columns:
    # Local CSV format detected
    print("‚úì Detected: Local CSV format")
    df = results_df.copy()

    # Standardize column names if needed
    if 'latency' in df.columns and 'latency_seconds' not in df.columns:
        df['latency_seconds'] = df['latency']

    if 'percent_error' not in df.columns and 'absolute_percent_error' in df.columns:
        df['percent_error'] = df['absolute_percent_error']

else:
    # Weave Export format detected
    print("‚úì Detected: Weave export format")
    df = results_df.copy()

    # Map nested columns to flat structure
    df['model'] = df['output.model'].fillna(df['inputs.model_name'])
    df['strategy'] = df['output.strategy'].fillna(df['inputs.strategy_name'])
    df['latency_seconds'] = df['output.latency'].fillna(df['summary.weave.latency_ms'] / 1000)
    df['tokens'] = df['output.tokens'].fillna(0)
    df['estimated_value'] = df['output.estimated_value'].fillna(0)
    df['actual_price'] = df['output.actual_price'].fillna(df['inputs.actual_price'])
    df['percent_error'] = df['output.percent_error'].fillna(df['output.absolute_percent_error'])
    df['absolute_error'] = df['output.absolute_error'].fillna(0)
    df['success'] = df['output.success'].fillna(True)

# Calculate derived metrics
df['percent_error_abs'] = df['percent_error'].abs()

# Merge cost data from Weave export if available
if not weave_df.empty:
    cost_cols = [
        'summary.weave.costs.claude-sonnet-4-5-20250929.completion_tokens_total_cost',
        'summary.weave.costs.claude-sonnet-4-5-20250929.prompt_tokens_total_cost'
    ]

    if all(col in weave_df.columns for col in cost_cols):
        if len(df) == len(weave_df):
            df['claude_completion_cost'] = weave_df[cost_cols[0]]
            df['claude_prompt_cost'] = weave_df[cost_cols[1]]
            df['total_cost'] = (
                df['claude_completion_cost'].fillna(0) +
                df['claude_prompt_cost'].fillna(0)
            )
            print("‚úì Cost data merged successfully")
        else:
            print("‚ö† WARNING: Row count mismatch - skipping cost merge")

# Filter to successful predictions
df_all = df.copy()  # Keep original for success rate analysis
df = df[df['success'] == True].copy()
df = df[df['estimated_value'] > 0].copy()

print(f"\n‚úì Data preprocessing complete")
print(f"  Total records: {len(df_all)}")
print(f"  Successful predictions: {len(df)} ({len(df)/len(df_all)*100:.1f}%)")
print(f"  Models: {sorted(df['model'].unique())}")
print(f"  Strategies: {sorted(df['strategy'].unique())}")
print(f"  Properties evaluated: {df['actual_price'].nunique()}")

# Cell 4: Overall Summary Statistics
# ==============================================================================
print("\n" + "="*80)
print("OVERALL SUMMARY STATISTICS")
print("="*80)

print(f"\nTotal Predictions: {len(df):,}")
print(f"Mean Absolute Error: ${df['absolute_error'].mean():,.2f}")
print(f"Median Absolute Error: ${df['absolute_error'].median():,.2f}")
print(f"Std Dev Absolute Error: ${df['absolute_error'].std():,.2f}")
print(f"Mean Percent Error: {df['percent_error_abs'].mean():.2f}%")
print(f"Median Percent Error: {df['percent_error_abs'].median():.2f}%")
print(f"Mean Latency: {df['latency_seconds'].mean():.2f}s")
print(f"Median Latency: {df['latency_seconds'].median():.2f}s")
print(f"Mean Tokens: {df['tokens'].mean():.0f}")
print(f"Median Tokens: {df['tokens'].median():.0f}")

# Cell 5: Performance by Model
# ==============================================================================
print("\n" + "="*80)
print("PERFORMANCE BY MODEL")
print("="*80)

model_stats = df.groupby('model').agg({
    'percent_error_abs': 'mean',
    'absolute_error': ['mean', 'median', 'std'],
    'latency_seconds': 'mean',
    'tokens': 'mean',
    'estimated_value': 'count'
}).round(2)

model_stats.columns = [
    'Mean % Error',
    'Mean $ Error',
    'Median $ Error',
    'Std $ Error',
    'Avg Latency (s)',
    'Avg Tokens',
    'Count'
]

print("\n", model_stats)

# Cell 6: Performance by Strategy
# ==============================================================================
print("\n" + "="*80)
print("PERFORMANCE BY STRATEGY")
print("="*80)

strategy_stats = df.groupby('strategy').agg({
    'percent_error_abs': 'mean',
    'absolute_error': ['mean', 'median', 'std'],
    'latency_seconds': 'mean',
    'tokens': 'mean',
    'estimated_value': 'count'
}).round(2)

strategy_stats.columns = [
    'Mean % Error',
    'Mean $ Error',
    'Median $ Error',
    'Std $ Error',
    'Avg Latency (s)',
    'Avg Tokens',
    'Count'
]

print("\n", strategy_stats.sort_values('Mean % Error'))

# Cell 7: Model-Strategy Performance Matrix
# ==============================================================================
print("\n" + "="*80)
print("MODEL-STRATEGY PERFORMANCE MATRIX")
print("="*80)

perf_matrix = df.groupby(['model', 'strategy']).agg({
    'percent_error_abs': ['mean', 'std'],
    'absolute_error': 'mean',
    'latency_seconds': 'mean',
    'tokens': 'mean',
    'estimated_value': 'count'
}).round(2)

perf_matrix.columns = ['% Error', 'Std Dev %', '$ Error', 'Latency (s)', 'Tokens', 'Count']
print("\n", perf_matrix)

# Cell 8: Visualization 1 - Error Comparison
# ==============================================================================
print("\n" + "="*80)
print("GENERATING VISUALIZATIONS...")
print("="*80)

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=(
        'Mean Percent Error by Model',
        'Mean Percent Error by Strategy',
        'Absolute Error Distribution by Model',
        'Absolute Error Distribution by Strategy'
    ),
    specs=[[{'type': 'bar'}, {'type': 'bar'}],
           [{'type': 'box'}, {'type': 'box'}]]
)

# Plot 1: Mean % Error by Model
model_error = df.groupby('model')['percent_error_abs'].mean().reset_index()
model_error = model_error.sort_values('percent_error_abs')
fig.add_trace(
    go.Bar(
        x=model_error['model'],
        y=model_error['percent_error_abs'],
        marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'],
        text=model_error['percent_error_abs'].round(2),
        textposition='outside',
        name='Model Error'
    ),
    row=1, col=1
)

# Plot 2: Mean % Error by Strategy
strategy_error = df.groupby('strategy')['percent_error_abs'].mean().reset_index()
strategy_error = strategy_error.sort_values('percent_error_abs')
fig.add_trace(
    go.Bar(
        x=strategy_error['strategy'],
        y=strategy_error['percent_error_abs'],
        marker_color=['#95E1D3', '#F38181', '#AA96DA', '#FCBAD3', '#FFFFD2'],
        text=strategy_error['percent_error_abs'].round(2),
        textposition='outside',
        name='Strategy Error'
    ),
    row=1, col=2
)

# Plot 3: Box plot by Model
for model in df['model'].unique():
    model_data = df[df['model'] == model]['absolute_error']
    fig.add_trace(
        go.Box(y=model_data, name=model, showlegend=False),
        row=2, col=1
    )

# Plot 4: Box plot by Strategy
for strategy in df['strategy'].unique():
    strategy_data = df[df['strategy'] == strategy]['absolute_error']
    fig.add_trace(
        go.Box(y=strategy_data, name=strategy, showlegend=False),
        row=2, col=2
    )

fig.update_xaxes(title_text="Model", row=1, col=1)
fig.update_xaxes(title_text="Strategy", row=1, col=2)
fig.update_xaxes(title_text="Model", row=2, col=1)
fig.update_xaxes(title_text="Strategy", row=2, col=2)
fig.update_yaxes(title_text="Mean % Error", row=1, col=1)
fig.update_yaxes(title_text="Mean % Error", row=1, col=2)
fig.update_yaxes(title_text="Absolute Error ($)", row=2, col=1)
fig.update_yaxes(title_text="Absolute Error ($)", row=2, col=2)

fig.update_layout(
    height=800,
    title_text="<b>Figure 1: Error Analysis Across Models and Strategies</b>",
    showlegend=False
)
fig.show()

# Cell 9: Visualization 2 - Latency Analysis
# ==============================================================================
fig = make_subplots(
    rows=1, cols=3,
    subplot_titles=(
        'Latency by Model',
        'Latency by Strategy',
        'Latency Distribution'
    ),
    specs=[[{'type': 'box'}, {'type': 'box'}, {'type': 'histogram'}]]
)

# Latency by Model
for model in df['model'].unique():
    model_data = df[df['model'] == model]['latency_seconds']
    fig.add_trace(
        go.Box(y=model_data, name=model),
        row=1, col=1
    )

# Latency by Strategy
for strategy in df['strategy'].unique():
    strategy_data = df[df['strategy'] == strategy]['latency_seconds']
    fig.add_trace(
        go.Box(y=strategy_data, name=strategy, showlegend=False),
        row=1, col=2
    )

# Histogram
fig.add_trace(
    go.Histogram(
        x=df['latency_seconds'],
        nbinsx=30,
        marker_color='lightblue',
        showlegend=False
    ),
    row=1, col=3
)

fig.update_xaxes(title_text="Model", row=1, col=1)
fig.update_xaxes(title_text="Strategy", row=1, col=2)
fig.update_xaxes(title_text="Latency (seconds)", row=1, col=3)
fig.update_yaxes(title_text="Latency (seconds)", row=1, col=1)
fig.update_yaxes(title_text="Latency (seconds)", row=1, col=2)
fig.update_yaxes(title_text="Frequency", row=1, col=3)

fig.update_layout(
    height=400,
    title_text="<b>Figure 2: Latency Analysis</b>",
    showlegend=True
)
fig.show()

# Cell 10: Visualization 3 - Token Usage Analysis
# ==============================================================================
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=('Token Usage by Model', 'Token Usage by Strategy')
)

# Token usage by Model
model_tokens = df.groupby('model')['tokens'].mean().reset_index()
model_tokens = model_tokens.sort_values('tokens')
fig.add_trace(
    go.Bar(
        x=model_tokens['model'],
        y=model_tokens['tokens'],
        marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'],
        text=model_tokens['tokens'].round(0),
        textposition='outside'
    ),
    row=1, col=1
)

# Token usage by Strategy
strategy_tokens = df.groupby('strategy')['tokens'].mean().reset_index()
strategy_tokens = strategy_tokens.sort_values('tokens')
fig.add_trace(
    go.Bar(
        x=strategy_tokens['strategy'],
        y=strategy_tokens['tokens'],
        marker_color=['#95E1D3', '#F38181', '#AA96DA', '#FCBAD3', '#FFFFD2'],
        text=strategy_tokens['tokens'].round(0),
        textposition='outside'
    ),
    row=1, col=2
)

fig.update_xaxes(title_text="Model", row=1, col=1)
fig.update_xaxes(title_text="Strategy", row=1, col=2)
fig.update_yaxes(title_text="Average Tokens", row=1, col=1)
fig.update_yaxes(title_text="Average Tokens", row=1, col=2)

fig.update_layout(
    height=400,
    title_text="<b>Figure 3: Token Usage Analysis</b>",
    showlegend=False
)
fig.show()

# Cell 11: Visualization 4 - Performance Heatmap
# ==============================================================================
heatmap_data = df.groupby(['model', 'strategy'])['percent_error_abs'].mean().reset_index()
heatmap_pivot = heatmap_data.pivot(index='model', columns='strategy', values='percent_error_abs')

fig = go.Figure(data=go.Heatmap(
    z=heatmap_pivot.values,
    x=heatmap_pivot.columns,
    y=heatmap_pivot.index,
    colorscale='RdYlGn_r',
    text=np.round(heatmap_pivot.values, 2),
    texttemplate='%{text}%',
    textfont={"size": 12},
    colorbar=dict(title="Mean % Error")
))

fig.update_layout(
    title='<b>Figure 4: Performance Heatmap - Model vs Strategy (Mean % Error)</b>',
    xaxis_title='Strategy',
    yaxis_title='Model',
    height=450
)
fig.show()

# Cell 12: Visualization 5 - Relationship Analysis
# ==============================================================================
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=(
        'Latency vs Error',
        'Tokens vs Error',
        'Latency vs Tokens',
        'Actual Price vs Error'
    ),
    specs=[[{'type': 'scatter'}, {'type': 'scatter'}],
           [{'type': 'scatter'}, {'type': 'scatter'}]]
)

colors = {'claude': '#FF6B6B', 'llama_aws': '#4ECDC4', 'mistral_aws': '#45B7D1'}

# Plot 1: Latency vs Error
for model in df['model'].unique():
    model_data = df[df['model'] == model]
    fig.add_trace(
        go.Scatter(
            x=model_data['latency_seconds'],
            y=model_data['percent_error_abs'],
            mode='markers',
            name=model,
            marker=dict(color=colors.get(model, '#999'), size=5, opacity=0.6)
        ),
        row=1, col=1
    )

# Plot 2: Tokens vs Error
for model in df['model'].unique():
    model_data = df[df['model'] == model]
    fig.add_trace(
        go.Scatter(
            x=model_data['tokens'],
            y=model_data['percent_error_abs'],
            mode='markers',
            name=model,
            marker=dict(color=colors.get(model, '#999'), size=5, opacity=0.6),
            showlegend=False
        ),
        row=1, col=2
    )

# Plot 3: Latency vs Tokens
for model in df['model'].unique():
    model_data = df[df['model'] == model]
    fig.add_trace(
        go.Scatter(
            x=model_data['latency_seconds'],
            y=model_data['tokens'],
            mode='markers',
            name=model,
            marker=dict(color=colors.get(model, '#999'), size=5, opacity=0.6),
            showlegend=False
        ),
        row=2, col=1
    )

# Plot 4: Actual Price vs Error
for model in df['model'].unique():
    model_data = df[df['model'] == model]
    fig.add_trace(
        go.Scatter(
            x=model_data['actual_price'],
            y=model_data['absolute_error'],
            mode='markers',
            name=model,
            marker=dict(color=colors.get(model, '#999'), size=5, opacity=0.6),
            showlegend=False
        ),
        row=2, col=2
    )

fig.update_xaxes(title_text="Latency (s)", row=1, col=1)
fig.update_xaxes(title_text="Tokens", row=1, col=2)
fig.update_xaxes(title_text="Latency (s)", row=2, col=1)
fig.update_xaxes(title_text="Actual Price ($)", row=2, col=2)
fig.update_yaxes(title_text="% Error", row=1, col=1)
fig.update_yaxes(title_text="% Error", row=1, col=2)
fig.update_yaxes(title_text="Tokens", row=2, col=1)
fig.update_yaxes(title_text="Absolute Error ($)", row=2, col=2)

fig.update_layout(
    height=800,
    title_text="<b>Figure 5: Relationship Analysis</b>",
    showlegend=True
)
fig.show()

# Cell 13: Visualization 6 - Cost Analysis (Claude only)
# ==============================================================================
if 'total_cost' in df.columns:
    claude_data = df[df['model'] == 'claude'].copy()

    if not claude_data.empty and claude_data['total_cost'].sum() > 0:
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=('Cost per Call by Strategy', 'Cost vs Performance')
        )

        # Cost by Strategy
        cost_by_strategy = claude_data.groupby('strategy')['total_cost'].mean().reset_index()
        cost_by_strategy = cost_by_strategy.sort_values('total_cost')
        fig.add_trace(
            go.Bar(
                x=cost_by_strategy['strategy'],
                y=cost_by_strategy['total_cost'] * 1000,  # Convert to cents
                text=(cost_by_strategy['total_cost'] * 1000).round(2),
                textposition='outside',
                marker_color='#FF6B6B'
            ),
            row=1, col=1
        )

        # Cost vs Error
        fig.add_trace(
            go.Scatter(
                x=claude_data['total_cost'] * 1000,
                y=claude_data['percent_error_abs'],
                mode='markers',
                marker=dict(size=8, color='#FF6B6B', opacity=0.6)
            ),
            row=1, col=2
        )

        fig.update_xaxes(title_text="Strategy", row=1, col=1)
        fig.update_xaxes(title_text="Cost per Call (cents)", row=1, col=2)
        fig.update_yaxes(title_text="Cost per Call (cents)", row=1, col=1)
        fig.update_yaxes(title_text="% Error", row=1, col=2)

        fig.update_layout(
            height=400,
            title_text="<b>Figure 6: Cost Analysis (Claude)</b>",
            showlegend=False
        )
        fig.show()

        print(f"\nClaude Cost Summary:")
        print(f"  Average cost per call: ${claude_data['total_cost'].mean():.4f}")
        print(f"  Total cost: ${claude_data['total_cost'].sum():.4f}")
        print(f"  Min cost: ${claude_data['total_cost'].min():.4f}")
        print(f"  Max cost: ${claude_data['total_cost'].max():.4f}")

# Cell 14: Visualization 7 - Success Rate Analysis
# ==============================================================================
success_rate = df_all.groupby(['model', 'strategy'])['success'].agg(['sum', 'count']).reset_index()
success_rate['success_rate'] = (success_rate['sum'] / success_rate['count'] * 100).round(2)

fig = px.bar(
    success_rate,
    x='strategy',
    y='success_rate',
    color='model',
    barmode='group',
    title='<b>Figure 7: Success Rate by Model and Strategy</b>',
    labels={'success_rate': 'Success Rate (%)', 'strategy': 'Strategy'},
    color_discrete_map={
        'claude': '#FF6B6B',
        'llama_aws': '#4ECDC4',
        'mistral_aws': '#45B7D1'
    }
)
fig.update_layout(height=500)
fig.show()

print("\nSuccess Rates by Model and Strategy:")
success_pivot = success_rate.pivot(index='strategy', columns='model', values='success_rate')
print(success_pivot)

# Cell 15: Statistical Analysis
# ==============================================================================
print("\n" + "="*80)
print("STATISTICAL ANALYSIS")
print("="*80)

# 1. ANOVA: Model Differences
print("\n1. ANOVA TEST: Model Differences")
model_groups = [
    df[df['model'] == model]['percent_error_abs']
    for model in df['model'].unique()
]
f_stat_model, p_value_model = stats.f_oneway(*model_groups)
print(f"   F-statistic: {f_stat_model:.4f}")
print(f"   P-value: {p_value_model:.6f}")
print(f"   Result: {'‚úì Significant difference' if p_value_model < 0.05 else '‚úó No significant difference'} between models (Œ±=0.05)")

# 2. ANOVA: Strategy Differences
print("\n2. ANOVA TEST: Strategy Differences")
strategy_groups = [
    df[df['strategy'] == strategy]['percent_error_abs']
    for strategy in df['strategy'].unique()
]
f_stat_strategy, p_value_strategy = stats.f_oneway(*strategy_groups)
print(f"   F-statistic: {f_stat_strategy:.4f}")
print(f"   P-value: {p_value_strategy:.6f}")
print(f"   Result: {'‚úì Significant difference' if p_value_strategy < 0.05 else '‚úó No significant difference'} between strategies (Œ±=0.05)")

# 3. Correlation Analysis
print("\n3. CORRELATION ANALYSIS:")
correlation_vars = ['latency_seconds', 'tokens', 'percent_error_abs', 'absolute_error']
correlations = df[correlation_vars].corr()
print(correlations.round(3))

# 4. Effect Size Analysis
print("\n4. EFFECT SIZE ANALYSIS (Cohen's d):")
print("   Comparing best vs worst strategies:")
best_strategy = df.groupby('strategy')['percent_error_abs'].mean().idxmin()
worst_strategy = df.groupby('strategy')['percent_error_abs'].mean().idxmax()
best_data = df[df['strategy'] == best_strategy]['percent_error_abs']
worst_data = df[df['strategy'] == worst_strategy]['percent_error_abs']

mean_diff = best_data.mean() - worst_data.mean()
pooled_std = np.sqrt((best_data.std()**2 + worst_data.std()**2) / 2)
cohens_d = abs(mean_diff) / pooled_std

print(f"   Best: {best_strategy} (mean error: {best_data.mean():.2f}%)")
print(f"   Worst: {worst_strategy} (mean error: {worst_data.mean():.2f}%)")
print(f"   Cohen's d: {cohens_d:.3f}")
print(f"   Effect size: {'Small' if cohens_d < 0.5 else 'Medium' if cohens_d < 0.8 else 'Large'}")

# Cell 16: Key Insights and Recommendations
# ==============================================================================
print("\n" + "="*80)
print("KEY INSIGHTS AND RECOMMENDATIONS")
print("="*80)

# 1. Best Performing Combinations
print("\n1. BEST PERFORMING MODEL-STRATEGY COMBINATIONS:")
print("   (Ranked by lowest error)")
best_combos = df.groupby(['model', 'strategy'])['percent_error_abs'].mean().reset_index()
best_combos = best_combos.sort_values('percent_error_abs').head(5)
for i, row in best_combos.iterrows():
    print(f"   {i+1}. {row['model']:12s} + {row['strategy']:20s} = {row['percent_error_abs']:6.2f}% error")

# 2. Worst Performing Combinations
print("\n2. WORST PERFORMING MODEL-STRATEGY COMBINATIONS:")
print("   (Highest error - avoid these)")
worst_combos = df.groupby(['model', 'strategy'])['percent_error_abs'].mean().reset_index()
worst_combos = worst_combos.sort_values('percent_error_abs', ascending=False).head(5)
for i, row in worst_combos.iterrows():
    print(f"   {i+1}. {row['model']:12s} + {row['strategy']:20s} = {row['percent_error_abs']:6.2f}% error")

# 3. Efficiency Analysis
print("\n3. EFFICIENCY METRICS:")
print("   (Balancing speed, cost, and accuracy)")
efficiency = df.groupby('model').agg({
    'latency_seconds': 'mean',
    'tokens': 'mean',
    'percent_error_abs': 'mean'
}).round(2)
efficiency['speed_score'] = (100 / efficiency['latency_seconds']).round(2)
efficiency['accuracy_score'] = (100 / efficiency['percent_error_abs']).round(2)
efficiency['efficiency_index'] = (
    efficiency['speed_score'] * efficiency['accuracy_score']
).round(2)
efficiency = efficiency.sort_values('efficiency_index', ascending=False)
print(efficiency[['percent_error_abs', 'latency_seconds', 'tokens', 'efficiency_index']])

# 4. Strategy Variance Analysis
print("\n4. STRATEGY VARIANCE ANALYSIS:")
print("   (Lower variance = more consistent predictions)")
variance_analysis = df.groupby('strategy').agg({
    'absolute_error': ['mean', 'std', lambda x: x.std() / x.mean() if x.mean() > 0 else 0]
}).round(2)
variance_analysis.columns = ['Mean Error', 'Std Dev', 'Coeff of Variation']
variance_analysis = variance_analysis.sort_values('Coeff of Variation')
print(variance_analysis)

# 5. Price Range Performance
print("\n5. PERFORMANCE BY PRICE RANGE:")
df['price_range'] = pd.cut(
    df['actual_price'],
    bins=[0, 150000, 200000, 250000, 300000],
    labels=['<$150K', '$150-200K', '$200-250K', '>$250K']
)
price_performance = df.groupby('price_range')['percent_error_abs'].agg(['mean', 'count']).round(2)
price_performance.columns = ['Mean % Error', 'Count']
print(price_performance)

# Cell 17: Final Summary Report
# ==============================================================================
print("\n" + "="*80)
print("FINAL SUMMARY REPORT")
print("="*80)

print("\nüìä EXPERIMENT OVERVIEW:")
print(f"   ‚Ä¢ Total API calls: {len(df_all):,}")
print(f"   ‚Ä¢ Successful predictions: {len(df):,} ({len(df)/len(df_all)*100:.1f}%)")
print(f"   ‚Ä¢ Properties evaluated: {df['actual_price'].nunique()}")
print(f"   ‚Ä¢ Models tested: {len(df['model'].unique())}")
print(f"   ‚Ä¢ Strategies tested: {len(df['strategy'].unique())}")

print("\nüèÜ TOP PERFORMERS:")
best_model = model_stats['Mean % Error'].idxmin()
best_strategy = strategy_stats['Mean % Error'].idxmin()
best_combo = best_combos.iloc[0]
print(f"   ‚Ä¢ Best model: {best_model} ({model_stats.loc[best_model, 'Mean % Error']:.2f}% error)")
print(f"   ‚Ä¢ Best strategy: {best_strategy} ({strategy_stats.loc[best_strategy, 'Mean % Error']:.2f}% error)")
print(f"   ‚Ä¢ Best combination: {best_combo['model']} + {best_combo['strategy']} ({best_combo['percent_error_abs']:.2f}% error)")

print("\n‚ö° EFFICIENCY INSIGHTS:")
fastest_model = model_stats['Avg Latency (s)'].idxmin()
most_accurate = model_stats['Mean % Error'].idxmin()
print(f"   ‚Ä¢ Fastest model: {fastest_model} ({model_stats.loc[fastest_model, 'Avg Latency (s)']:.2f}s)")
print(f"   ‚Ä¢ Most accurate: {most_accurate} ({model_stats.loc[most_accurate, 'Mean % Error']:.2f}% error)")
print(f"   ‚Ä¢ Most token-efficient: {model_stats['Avg Tokens'].idxmin()} ({model_stats['Avg Tokens'].min():.0f} tokens)")

print("\nüí° KEY FINDINGS:")
print(f"   ‚Ä¢ Strategy impact: {strategy_stats['Mean % Error'].max() / strategy_stats['Mean % Error'].min():.1f}x difference between best/worst")
print(f"   ‚Ä¢ Model impact: {model_stats['Mean % Error'].max() / model_stats['Mean % Error'].min():.1f}x difference between best/worst")
print(f"   ‚Ä¢ Prompt engineering > model selection: {'Yes' if f_stat_strategy > f_stat_model else 'No'}")
if 'total_cost' in df.columns:
    avg_cost = df[df['model']=='claude']['total_cost'].mean()
    print(f"   ‚Ä¢ Average cost per prediction (Claude): ${avg_cost:.4f}")

print("\nüìã RECOMMENDATIONS:")
print("   1. Use few-shot strategy for best accuracy")
print("   2. Avoid role-playing strategy due to high variance")
print("   3. Choose model based on accuracy vs speed requirements:")
print(f"      - For accuracy: {most_accurate}")
print(f"      - For speed: {fastest_model}")
print("   4. Monitor extraction success rates by strategy")
print("   5. Consider price range when evaluating model performance")

print("\n" + "="*80)
print("‚úì COMPREHENSIVE ANALYSIS COMPLETE")
print("="*80)
print("\nAll visualizations and statistics have been generated.")
print("Scroll up to view detailed results and figures.")
print("="*80)